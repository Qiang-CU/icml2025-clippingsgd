<!DOCTYPE html>
<html>
<head>
  <meta charset="utf-8">
  <!-- Meta tags for social media banners, these should be filled in appropriatly as they are your "business card" -->
  <!-- Replace the content tag with appropriate information -->
  <meta name="description" content="Clipped SGD Algorithms for Performative Prediction: Tight Bounds for Clipping Bias and Remedies">
  <meta property="og:title" content="Clipped SGD Algorithms for Performative Prediction: Tight Bounds for Clipping Bias and Remedies"/>
  <meta property="og:description" content="This paper studies the convergence of clipped stochastic gradient descent (SGD) algorithms with decision-dependent data distribution."/>
  <meta property="og:url" content="https://qiang-cu.github.io/icml2025-clippingsgd/"/>
  <!-- Path to banner image, should be in the path listed below. Optimal dimenssions are 1200X630-->
  <meta property="og:image" content="static/image/your_banner_image.png" />
  <meta property="og:image:width" content="1200"/>
  <meta property="og:image:height" content="630"/>


  <meta name="twitter:title" content="Clipped SGD Algorithms for Performative Prediction">
  <meta name="twitter:description" content="Tight Bounds for Clipping Bias and Remedies">
  <!-- Path to banner image, should be in the path listed below. Optimal dimenssions are 1200X600-->
  <meta name="twitter:image" content="static/images/your_twitter_banner_image.png">
  <meta name="twitter:card" content="summary_large_image">
  <!-- Keywords for your paper to be indexed by-->
  <meta name="keywords" content="Performative Prediction, (Non)convex Optimization, Clipping SGD algorithm">
  <meta name="viewport" content="width=device-width, initial-scale=1">


  <title>Clipped SGD Algorithms for Performative Prediction</title>
  <link rel="icon" type="image/x-icon" href="static/images/qiang.png">
  <link href="https://fonts.googleapis.com/css?family=Google+Sans|Noto+Sans|Castoro"
  rel="stylesheet">

  <link rel="stylesheet" href="static/css/bulma.min.css">
  <link rel="stylesheet" href="static/css/bulma-carousel.min.css">
  <link rel="stylesheet" href="static/css/bulma-slider.min.css">
  <link rel="stylesheet" href="static/css/fontawesome.all.min.css">
  <link rel="stylesheet"
  href="https://cdn.jsdelivr.net/gh/jpswalsh/academicons@1/css/academicons.min.css">
  <link rel="stylesheet" href="static/css/index.css">

  <script src="https://ajax.googleapis.com/ajax/libs/jquery/3.5.1/jquery.min.js"></script>
  <script src="https://documentcloud.adobe.com/view-sdk/main.js"></script>
  <script defer src="static/js/fontawesome.all.min.js"></script>
  <script src="static/js/bulma-carousel.min.js"></script>
  <script src="static/js/bulma-slider.min.js"></script>
  <script src="static/js/index.js"></script>
  
  <!-- MathJax for rendering equations -->
  <script src="https://polyfill.io/v3/polyfill.min.js?features=es6"></script>
  <script id="MathJax-script" async src="https://cdn.jsdelivr.net/npm/mathjax@3/es5/tex-mml-chtml.js"></script>
  
  <!-- MathJax configuration with custom macros -->
  <script type="text/javascript">
    window.MathJax = {
      tex: {
        inlineMath: [['$', '$'], ['\\(', '\\)']],
        displayMath: [['$$', '$$'], ['\\[', '\\]']],
        processEscapes: true,
        packages: {'[+]': ['noerrors']},
        macros: {
          // Basic sets
          RR: '{\\mathbb{R}}',
          EE: '{\\mathbb{E}}',
          PP: '{\\mathbb{P}}',
          ZZ: '{\\mathbb{Z}}',
          NN: '{\\mathbb{N}}',
          QQ: '{\\mathbb{Q}}',
          CC: '{\\mathbb{C}}',
          
          // Operators
          tr: '{\\operatorname{tr}}',
          diag: '{\\operatorname{diag}}',
          rank: '{\\operatorname{rank}}',
          sgn: '{\\operatorname{sgn}}',
          spanop: '{\\operatorname{span}}',
          proj: '{\\operatorname{proj}}',
          proxb: '{\\operatorname{prox}}',
          sign: '{\\operatorname{sign}}',
          
          // Custom notations
          norm: ['{\\left\\lVert #1 \\right\\rVert}', 1],
          inner: ['{\\langle #1, #2 \\rangle}', 2],
          prm: '{\\bm{\\theta}}',
          grad: '{\\nabla}',
          bm: ['{\\boldsymbol{#1}}', 1],
          
          // Distributions
          Normal: '{\\mathcal{N}}',
          Uniform: '{\\mathcal{U}}',
          Bernoulli: '{\\mathcal{B}}',
          
          // Algorithm-specific
          clip: '{\\operatorname{clip}}',
          noiseop: '{\\mathcal{N}}',
          lossfn: '{\\ell}'
        }
      },
      options: {
        renderActions: {
          addMenu: []
        }
      }
    };
  </script>
  
</head>
<body>


  <section class="hero">
    <div class="hero-body">
      <div class="container is-max-desktop">
        <div class="columns is-centered">
          <div class="column has-text-centered">
            <h1 class="title is-1 publication-title">Clipped SGD Algorithms for Performative Prediction: Tight Bounds for Clipping Bias and Remedies</h1>
            <div class="is-size-5 publication-authors">
              <!-- Paper authors -->
              <span class="author-block">
                <a href="https://qiang-cu.github.io/" target="_blank">Qiang Li</a><sup>1</sup>,</span>
                <span class="author-block">
                  <a href="https://michalyem.github.io/" target="_blank">Michal Yemini</a><sup>2</sup>,</span>
                  <span class="author-block">
                    <a href="https://www1.se.cuhk.edu.hk/~htwai/" target="_blank">Hoi-To Wai</a><sup>1</sup>
                  </span>
                  </div>

                  <div class="is-size-5 publication-authors">
                    <span class="author-block"><sup>1</sup>Department of SEEM, The Chinese University of Hong Kong, Hong Kong<br>
                    <sup>2</sup>Faculty of Engineering, Bar-Ilan University, Israel<br>
                    International Conference on Machine Learning (ICML 2025) Main Conference</span>
                  </div>

                  <div class="is-flex is-justify-content-center mt-2 mb-3">
                    <div class="mx-2" style="height: 70px;">
                      <img src="static/images/CUHK.png" alt="CUHK Logo" style="height: 100%;">
                    </div>
                    <div class="mx-2" style="height: 70px;">
                      <img src="static/images/BIU.jpg" alt="BIU Logo" style="height: 100%;">
                    </div>
                  </div>

                  <div class="column has-text-centered">
                    <div class="publication-links">
                         <!-- Arxiv PDF link -->
                      <span class="link-block">
                        <a href="https://arxiv.org/pdf/2404.10995" target="_blank"
                        class="external-link button is-normal is-rounded is-dark">
                        <span class="icon">
                          <i class="fas fa-file-pdf"></i>
                        </span>
                        <span>Paper</span>
                      </a>
                    </span>

                    
                    <!-- Supplementary PDF link -->
                    <span class="link-block">
                      <a href="static/pdfs/Fancy_Posters_ICML_Workshop.pdf" target="_blank"
                      class="external-link button is-normal is-rounded is-dark">
                      <span class="icon">
                        <i class="fas fa-file-pdf"></i>
                      </span>
                      <span>Poster</span>
                      </a>
                    </span>

                  <!-- Github link -->
                  <span class="link-block">
                    <a href="https://github.com/Qiang-CU/ICML2024-ClipBias.git" target="_blank"
                    class="external-link button is-normal is-rounded is-dark">
                    <span class="icon">
                      <i class="fab fa-github"></i>
                    </span>
                    <span>Code</span>
                  </a>
                </span>

                <!-- 
                ArXiv abstract Link
                <span class="link-block">
                  <a href="https://arxiv.org/abs/2404.10995" target="_blank"
                  class="external-link button is-normal is-rounded is-dark">
                  <span class="icon">
                    <i class="ai ai-arxiv"></i>
                  </span>
                  <span>arXiv</span>
                  </a>
                </span>
                -->
              </div>
            </div>
          </div>
        </div>
      </div>
    </div>
  </div>
</section>


  <!-- Teaser video section completely removed -->

  <!-- Paper abstract -->
  <section class="section hero is-light" style="padding-top: 2rem; margin-top: -2rem;">
    <div class="container is-max-desktop">
      <div class="columns is-centered has-text-centered">
        <div class="column is-four-fifths">
          <h2 class="title is-3">Abstract</h2>
          <div class="content has-text-justified">
            <p>
              This paper studies the convergence of clipped stochastic gradient descent (SGD) algorithms with decision-dependent data distribution. Our setting is motivated by privacy preserving optimization algorithms that interact with performative data where the prediction models can influence future outcomes. This challenging setting involves the non-smooth clipping operator and non-gradient dynamics due to distribution shifts. We make two contributions in pursuit for a performative stable solution using clipped SGD algorithms. First, we characterize the clipping bias with projected clipped SGD (PCSGD) algorithm which is caused by the clipping operator that prevents PCSGD from reaching a stable solution. When the loss function is strongly convex, we quantify the lower and upper bounds for this clipping bias and demonstrate a bias amplification phenomenon with the sensitivity of data distribution. When the loss function is non-convex, we bound the magnitude of stationarity bias. Second, we propose remedies to mitigate the bias either by utilizing an optimal step size design for PCSGD, or to apply the recent DiceSGD algorithm <a href="https://arxiv.org/pdf/2311.14632" target="_blank" style="color: #3273dc; font-weight: bold;">[Zhang et al., 2023]</a>. Our analysis is also extended to show that the latter algorithm is free from clipping bias in the performative setting. Numerical experiments verify our findings.
            </p>
            
            <h3 class="title is-4 has-text-centered">Keywords</h3>
            <p class="has-text-centered">
              <span class="tag is-medium is-link is-light">Performative Prediction</span>
              <span class="tag is-medium is-link is-light">(Non)convex Optimization</span>
              <span class="tag is-medium is-link is-light">Clipping SGD Algorithm</span>
              <span class="tag is-medium is-link is-light">Differential Privacy</span>
              <span class="tag is-medium is-link is-light">Distribution Shift</span>
            </p>
          </div>
        </div>
      </div>
    </div>
  </section>
  <!-- End paper abstract -->

  <!-- Performative Prediction Section -->
  <section class="section">
    <div class="container is-max-desktop">
      <div class="columns is-centered">
        <div class="column is-four-fifths">
          <h2 class="title is-3 has-text-centered">Performative Prediction</h2>
          <div class="content has-text-justified">
            <p>
              <strong>Performative Prediction</strong> refers to a stochastic optimization problem in which the data distribution depends on the decision variable. This framework is especially relevant in a range of real-world applications:
            </p>
            <div class="has-text-centered my-5">
              <img src="static/images/perf pred.png" alt="Performative Prediction" style="max-width: 40%;">
            </div>
            
            <ul>
              <li><strong>Loan approval prediction</strong>: When training a classifier for loan applications, the published decision rule (e.g., from a bank) may influence how applicants behave. In anticipation of the classifier, applicants might manipulate their profiles before submission‚Äîsuch as by increasing credit card usage or making artificial transactions‚Äîto improve their chances of approval. This feedback loop can impact the convergence and stability of the learning algorithm itself.</li>
              <li><strong>Signal degeneracy in financial markets</strong>: As discussed in <a href="https://www.morganstanley.com/im/publication/insights/investment-insights/ii_builtforchangeshiftingfrombetatoalpha_us.pdf" target="_blank"><em>Shifting from Beta to Alpha</em></a>, traditional alpha factors often degrade under changing macroeconomic regimes, reducing model accuracy. Additionally, trading behavior influenced by predictive models can itself reshape market dynamics, affecting the distribution of financial data in future periods.</li>
            </ul>
            
            The optmization problem is formulated as following: 
            <p class="has-text-centered mt-4">
              <span class="is-size-5">$\min_{\prm \in \RR^d} \EE_{Z\sim \mathcal{D}(\prm)} [\lossfn(\prm; Z)]$</span>
            </p>
            where ${\cal D}(\prm)$ capture the distribution induced by decision $\prm$.
          </div>
          
        </div>
      </div>
    </div>
  </section>
  <!-- End Performative Prediction Section -->

  <!-- Research Motivation Section -->
  <section class="section">
    <div class="container is-max-desktop">
      <div class="columns is-centered">
        <div class="column is-four-fifths">
          <h2 class="title is-3 has-text-centered"> Algorithms: PCSGD and DiceSGD</h2>
          
          <div class="content has-text-justified">
            <h4 class="title is-4">Motivation</h4>
            Gradient clipping techniques is used to deal with multiple obstacles in
learning algorithms such as the need for privacy preservation <a href="https://arxiv.org/abs/1607.00133" target="_blank">[Abadi et al., 2016]</a>, dealing with gradient explosion in
non-smooth learning <a href="https://ntu-sp.primo.exlibrisgroup.com/discovery/fulldisplay/alma991013951869705146/65NTU_INST:65NTU_INST" target="_blank">[Shor, 2012]</a> such as training neural
networks <a href="https://www.fit.vut.cz/study/phd-thesis/283/.en" target="_blank">[Mikolov et al., 2012]</a>; [Zhang et al., 2020a], <a href="https://proceedings.neurips.cc/paper_files/paper/2015/file/934815ad542a4a7c5e8a2dfa04fea9f5-Paper.pdf" target="_blank">[Hazan et al., 2015]</a>, etc.
            
            <h4 class="title is-4 mt-5">Research Gap</h4>
            <p>
              Most existing works have focused on analyzing SGD algorithms with a smooth drift term. An open problem in the performative prediction literature is to analyze the behavior of <em>clipped SGD</em> algorithms which limit the magnitude of the stochastic gradient at every update step.
            </p>
            
            <h3 class="title is-3 mt-5">üìå Key Contributions</h3>
            
            <ul class="mt-4">
              <li class="mb-3">üîÑ <strong>Convergence Analysis under Performative Feedback:</strong> We analyze the convergence of PC-SGD when data distribution depends on the model's decision, showing convergence to a biased performative stable solution under both convex and non-convex settings.</li>
              
              <li class="mb-3">üìä <strong>Quantification of Clipping Bias:</strong> We characterize the asymptotic bias introduced by gradient clipping and show it is amplified by distribution shift and clipping sensitivity. A matching lower bound is provided in the strongly convex case.</li>
              
              <li class="mb-3">‚úÇÔ∏è <strong>Bias Mitigation via Double Clipping:</strong> We study the recently proposed DICE-SGD algorithm and prove that its double clipping scheme can eliminate or significantly reduce the bias, achieving near-exact convergence.</li>
              
              <li class="mb-3">üîí <strong>Privacy‚ÄìEfficiency Tradeoff:</strong> We identify a tradeoff between differential privacy guarantees and computational efficiency, which informs optimal step size selection in practice.</li>
            </ul>

            <h4 class="title is-4 mt-5">Theoretical Results</h4>
            <div class="table-container">
              <table class="table is-bordered is-striped is-hoverable is-fullwidth">
                <thead>
                  <tr>
                    <th>Algorithm</th>
                    <th>Loss Function</th>
                    <th>Convergence</th>
                    <th>Bias Level</th>
                  </tr>
                </thead>
                <tbody>
                  <tr>
                    <td>PCSGD</td>
                    <td>Strongly Convex</td>
                    <td>${\cal O}(1/t) + \text{bias}$</td>
                    <td><span style="color: #3273dc;">${\bf \Theta}( 1 / (\mu - L \beta)^2 )$</span></td>
                  </tr>
                  <tr>
                    <td>PCSGD</td>
                    <td>Non-Convex</td>
                    <td>${\cal O}(1/\sqrt{T}) + \text{bias}$</td>
                    <td>${\cal O}(\beta + \max\{G-c, 0\}^2)$</td>
                  </tr>
                  <tr>
                    <td>DiceSGD</td>
                    <td>Strongly Convex</td>
                    <td>${\cal O}(1/t)$</td>
                    <td>N/A</td>
                  </tr>
                  <tr>
                    <td>DiceSGD</td>
                    <td>Non-Convex</td>
                    <td>${\cal O}(1/\sqrt{T}) + \text{bias}$</td>
                    <td>${\cal O}(\beta)$</td>
                  </tr>
                </tbody>
              </table>
              <p class="is-italic has-text-centered">Note: $\beta$ represents the strength of distribution shift. $G$ is bound of gradient and $c$ is the clipping threshold, $t$ is the iteration index, $T$ is the total number of iterations.</p>
            </div>
            
            <h4 class="title is-4 mt-5">PCSGD Algorithm</h4>
            <div class="box has-background-light">
              <div class="content">
                <p>
                  <strong>Deploy:</strong> $Z_{t+1} \sim {\cal D}( \prm_t )$
                </p>
                <p>
                  <strong>Update:</strong> $\prm_{t+1} = {\cal P}_{\cal X} \big(\prm_t - \gamma_{t+1} \clip_c( \grad \lossfn(\prm_t; Z_{t+1})) \big)$
                </p>
                <p>
                  where ${\cal P}_{\cal X}(\cdot)$ denotes the Euclidean projection operator onto ${\cal X}$, and $\clip_c(\cdot)$ is the clipping operator: for any $\bm{g} \in \RR^d$,
                </p>
                <p class="has-text-centered">
                  $\clip_{c} ( \bm{g} ): \bm{g} \in \RR^d \mapsto \min\left\{1, {c} / {\norm{\bm{g}}_2} \right\} \bm{g}$
                </p>
                <p>
                  where $c>0$ is a clipping parameter.
                </p>
              </div>
            </div>
            
            <h4 class="title is-4 mt-5">DiceSGD Algorithm</h4>
            <div class="box has-background-light">
              <div class="content">
                <p><strong>Input:</strong> Parameters $C_1$, $C_2$, step sizes, privacy parameters $\varepsilon$, $\delta$, initialization $\prm_0$, $e_0 = \bm{0}$.</p>
                <p><strong>For</strong> $t=0$ to $T-1$:</p>
                <ol>
                  <li>Draw new sample $Z_{t+1}\sim {\cal D}(\prm_t)$ and Gaussian noise $\zeta_{t+1} \sim {\cal N}(0, \sigma^2 \bm{I})$.</li>
                  <li>$v_{t+1} = \clip_{C_1}(\grad \lossfn(\prm_t; Z_{t+1})) + \clip_{C_2}(e_t)$.</li>
                  <li>$\prm_{t+1} = \prm_t - \gamma_{t+1} (v_{t+1} + \zeta_{t+1})$</li>
                  <li>$e_{t+1} = e_t + \grad \lossfn(\prm_t; Z_{t+1}) - v_{t+1}$.</li>
                </ol>
                <p><strong>Output:</strong> Last iterate $\prm_{T}$.</p>
              </div>
            </div>
            
            
          </div>
        </div>
      </div>
    </div>
  </section>
  <!-- End Research Motivation Section -->
  
  <!-- Simulation Results Section -->
  <section class="section">
    <div class="container is-max-desktop">
      <div class="columns is-centered">
        <div class="column is-four-fifths">
          <h2 class="title is-3 has-text-centered">Simulation Results</h2>
          
          <div class="content has-text-justified">
            <p>
              We consider a scalar performative risk optimization problem with synthetic data:
            </p>
            <p class="has-text-centered">
              $\min_{\prm \in {\cal X}}~ \EE_{z\sim {\cal D}(\prm)}[(\prm + az)^2/2]$
            </p>
            <p>
              where ${\cal D}(\prm)$ is a uniform distribution over the data points $\{ b \tilde{Z}_i - \beta \prm \}_{i=1}^m$ such that $\tilde{Z}_i \sim {\cal B}(p)$ is Bernoulli distribution.
            </p>
            
            <h5 class="title is-4 mt-5">Experimental Results</h5>
            
            <div class="columns is-centered mt-4">
              <div class="column">
                <div class="columns is-centered">
                  <div class="column is-4">
                    <figure class="image">
                      <img src="static/images/quad_min.png" alt="Quadratic Minimization" style="width: 100%; height: auto;">
                    </figure>
                  </div>
                  <div class="column is-4">
                    <figure class="image">
                      <img src="static/images/eps_vs_utility.png" alt="Privacy vs Utility" style="width: 100%; height: auto;">
                    </figure>
                  </div>
                  <div class="column is-4">
                    <figure class="image">
                      <img src="static/images/pb_vs_utility.png" alt="Parameter vs Utility" style="width: 100%; height: auto;">
                    </figure>
                  </div>
                </div>
                <div class="content has-text-centered mt-4">
                  <p class="subtitle is-6">
                    <strong>Figure:</strong> (left) The performative stability gap $\|\prm_t - \prm_{PS}\|^2$. (middle) Trade off between privacy budget $\varepsilon$ and bias. (right) Bias amplification effect due to $\beta$.
                  </p>
                </div>
              </div>
            </div>
            
            <h4 class="title is-4 mt-5">Observations</h4>
            <ul>
              <li>PCSGD cannot converge to $\prm_{PS}$ due to the clipping bias which increases as $\beta \uparrow$.</li>
              <li>DiceSGD finds a <em>bias-free</em> solution as it converges to $\prm_{PS}$ at rate ${\cal O}(1/t)$.</li>
              <li>As the privacy budget decreases $\varepsilon \downarrow 0$ or $\beta \uparrow \frac{\mu}{L}$, the bias increases.</li>
            </ul>
            
            
          </div>
        </div>
      </div>
    </div>
  </section>
  <!-- End Simulation Results Section -->
  
  <!-- Contact Section -->
  <section class="section">
    <div class="container is-max-desktop">
      <div class="columns is-centered">
        <div class="column is-four-fifths">
          <div class="content has-text-centered">
            <h2 class="title is-4">Contact</h2>
            <p>
              Welcome to check our paper for more details of this research work. If you have any comments or questions, please feel free to contact us.
            </p>
          </div>
        </div>
      </div>
    </div>
  </section>
  <!-- End Contact Section -->

  <!--BibTex citation -->
  <section class="section" id="BibTeX">
    <div class="container is-max-desktop content">
      <p>If you find our paper and repo useful, please cite us as follows:</p>
      <div class="notification is-light">
        <button class="button is-small is-info is-light is-pulled-right" onclick="copyToClipboard()">
          <span class="icon">
            <i class="fas fa-copy"></i>
          </span>
          <span>Copy</span>
        </button>
        <pre style="background-color:transparent; border: none;" id="bibtex"><code>@inproceedings{li2025clipped,
  title={Clipped SGD Algorithms for Performative Prediction: Tight Bounds for Clipping Bias and Remedies},
  author={Li, Qiang and Yemini, Michal and Wai, Hoi-To},
  booktitle={International Conference on Machine Learning (ICML)},
  year={2025},
  organization={PMLR}
}</code></pre>
        <script>
          function copyToClipboard() {
            const bibtexText = document.getElementById('bibtex').textContent;
            navigator.clipboard.writeText(bibtexText)
              .then(() => {
                const button = document.querySelector('.button.is-info.is-light');
                button.classList.add('is-success');
                button.classList.remove('is-info');
                setTimeout(() => {
                  button.classList.remove('is-success');
                  button.classList.add('is-info');
                }, 2000);
              })
              .catch(err => {
                console.error('Failed to copy text: ', err);
              });
          }
        </script>
      </div>
    </div>
  </section>
  <!--End BibTex citation -->


  <footer class="footer">
  <div class="container">
    <div class="columns is-centered">
      <div class="column is-4">
        <div class="content">
          <p>
            Thanks to <a href="https://github.com/eliahuhorwitz/Academic-project-page-template" target="_blank">Academic Project Page Template</a> and <a href="https://nerfies.github.io" target="_blank">Nerfies</a>.
          </p>
        </div>
      </div>
      <div class="column is-4">
        <div class="content has-text-centered">
          <!-- Statcounter visitor stats -->
          <script type="text/javascript">
            var sc_project=13135718; 
            var sc_invisible=0; 
            var sc_security="a00f39cc"; 
            var scJsHost = "https://";
            document.write("<sc"+"ript type='text/javascript' src='" + scJsHost+
            "statcounter.com/counter/counter.js'></"+"script>");
          </script>
          <noscript><div class="statcounter"><a title="Web Analytics Made Easy - Statcounter" href="https://statcounter.com/" target="_blank"><img class="statcounter" src="https://c.statcounter.com/13135718/0/a00f39cc/0/" alt="Web Analytics Made Easy - Statcounter" referrerPolicy="no-referrer-when-downgrade"></a></div></noscript>
          <!-- End of Statcounter Code -->
          <p><small>Visitor Statistics</small></p>
        </div>
      </div>
    </div>
  </div>
</footer>

<!-- End of Statcounter Code -->

  </body>
  </html>
